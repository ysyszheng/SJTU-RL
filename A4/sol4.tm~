<TeXmacs|2.1.2>

<style|generic>

<\body>
  <doc-data|<doc-title|Assignment 4>|<doc-author|<author-data|<author-name|yusen
  zheng>|<author-misc|Student ID: 520021911173>|<author-email|zys0794@sjtu.edu.cn>|<\author-affiliation>
    <date|>
  </author-affiliation>>>>

  <section|Introduction>

  In this assignment, we implemented and compared the performance of DQN and
  Dueling-DQN and test them in a classical RL control environment \V
  MountainCar.

  <section|Model Architecture>

  <subsection|Deep Q-learning Network>

  In this study, we employed Deep Q-learning Network (DQN) algorithms to
  train an agent for solving the MountainCar problem, which is a classic
  reinforcement learning task. DQN is a model-free, and value-based deep
  reinforcement learning algorithm. It combines Q-learning with deep neural
  networks to approximate the action-value function, which maps states to
  action values.

  The DQN algorithm follows a Q-learning approach, where the agent learns an
  action-value function, denoted as <math|Q<around*|(|s,a|)>>, that maps
  states <math|s> to action values <math|a>. The agent uses an
  <math|\<epsilon\>>-greedy exploration strategy, where it selects the action
  with the highest Q-value with probability
  <math|<around*|(|1-\<epsilon\>|)>>, and selects a random action with
  probability <math|\<epsilon\>>, in order to balance exploration and
  exploitation. The DQN algorithm uses a replay buffer to store and sample
  experiences, and updates the neural network weights using an \ optimizer to
  minimize the mean squared error (MSE) loss between the predicted Q-values
  and the target Q-values. The predicted Q-values and target Q-value at
  iteration <math|i> is:

  <\eqnarray*>
    <tformat|<table|<row|<cell|>|<cell|y<rsub|i><rsup|predict>=Q<around*|(|s<rsub|i>,a;\<theta\>|)>>|<cell|>>|<row|<cell|>|<cell|y<rsub|i><rsup|target>=r<rsub|i>+\<gamma\>
    max<rsub|a<rprime|'>> <wide|Q|^> <around*|(|s<rsub|i+1>,a<rprime|'>;\<theta\><rsup|->|)>>|<cell|>>>>
  </eqnarray*>

  and the loss function is:

  <\equation*>
    L<around*|(|\<theta\>|)>=\<bbb-E\><around*|[|<around*|(|y<rsub|i><rsup|target>-y<rsub|i><rsup|predict>|)><rsup|2>|]>
  </equation*>

  where <math|\<theta\>> and <math|\<theta\><rsup|->> denoted the parameters
  of <math|Q> network and target <math|<wide|Q|^>> network. While training,
  using Q-network to update the target Q-network every <math|C> step. The
  formal description is shown in Algotirhm <reference|alg1>.

  <\algorithm>
    <label|alg1>Initialize replay memory <math|D> to capacity <math|N>

    Initialize action-value function <math|Q> with random weights
    <math|\<theta\>>

    Initialize target action-value function <math|<wide|Q|^>> with weights
    <math|\<theta\><rsup|->=\<theta\>>

    <with|font-series|bold|For> <math|episode=1,M> <with|font-series|bold|do>

    <space|1em>Initialize begining state <math|s<rsub|1>>

    <space|1em><with|font-series|bold|For> <math|t=1,T>
    <with|font-series|bold|do>

    <space|3em>With probability <math|\<epsilon\>> select a random action
    <math|a<rsub|t>>

    <space|3em>otherwise select <math|a<rsub|t>=argmax<rsub|a>
    Q<around*|(|\<phi\><around*|(|s<rsub|t>|)>,a;\<theta\>|)>>

    <space|3em>Execute action at in emulator and observe reward
    <math|r<rsub|t>> and next state <math|s<rsub|t+1>>

    <space|3em>Store transition <math|<around*|(|s<rsub|t>,a<rsub|t>,r<rsub|t>,s<rsub|t+1>|)>>
    in <math|D>

    <space|3em>Sample random minibatch of transition
    <math|<around*|(|s<rsub|j>,a<rsub|j>,r<rsub|j>,s<rsub|j+1>|)>> from
    <math|D>

    <space|3em>Set <math|y<rsub|j>=<choice|<tformat|<table|<row|<cell|r<rsub|j>>|<cell|<text|,
    if episode terminates at step >j+1>>|<row|<cell|r<rsub|j>+\<gamma\>
    max<rsub|a<rprime|'>> <wide|Q|^> <around*|(|s<rsub|j+1>,a<rprime|'>;\<theta\><rsup|->|)>>|<cell|<text|,
    otherwise>>>>>>>

    <space|3em>Perform a gradient descent step on
    <math|<around*|(|y<rsub|j>-Q <around*|(|s<rsub|j>,a<rsub|j>;\<theta\>|)>|)><rsup|2>>
    w.r.t. the networ parameter <math|\<theta\>>

    <space|3em>Every <math|C> steps reset <math|<wide|Q|^>=Q>

    <space|1em><with|font-series|bold|End For>

    <with|font-series|bold|End For>
  </algorithm>

  <subsection|Double Deep Q-learning Network>

  We also employed a kind of improved DQN \V Double Deep Q-learning Network
  (DDQN). In Q-learning and DQN, the max operator uses the same values to
  both select and evaluate an action. This can therefore lead to
  overoptimistic value estimates. To mitigate this problem, DDQN first finds
  the optimal action by applying <math|Q> network:

  <\equation*>
    a<rsup|max><around*|(|s<rsub|i+1>;\<theta\>|)>=argmax<rsub|a<rprime|'>>
    Q<around*|(|s<rsub|i+1>,a<rprime|'>;\<theta\>|)>
  </equation*>

  and then calculates target Q-value by applying target <math|<wide|Q|^>>
  network:

  <\equation*>
    y<rsub|i><rsup|target>=r<rsub|i>+\<gamma\> <wide|Q|^>
    <around*|(|s<rsub|i+1>,a<rsup|max><around*|(|s<rsub|i+1>;\<theta\>|)>;\<theta\><rsup|->|)>
  </equation*>

  The formal description is shown in Algorithm <reference|alg2>.

  <\algorithm>
    <label|alg2>Initialize replay memory <math|D> to capacity <math|N>

    Initialize action-value function <math|Q> with random weights
    <math|\<theta\>>

    Initialize target action-value function <math|<wide|Q|^>> with weights
    <math|\<theta\><rsup|->=\<theta\>>

    <with|font-series|bold|For> <math|episode=1,M> <with|font-series|bold|do>

    <space|1em>Initialize begining state <math|s<rsub|1>>

    <space|1em><with|font-series|bold|For> <math|t=1,T>
    <with|font-series|bold|do>

    <space|3em>With probability <math|\<epsilon\>> select a random action
    <math|a<rsub|t>>

    <space|3em>otherwise select <math|a<rsub|t>=argmax<rsub|a>
    Q<around*|(|\<phi\><around*|(|s<rsub|t>|)>,a;\<theta\>|)>>

    <space|3em>Execute action at in emulator and observe reward
    <math|r<rsub|t>> and next state <math|s<rsub|t+1>>

    <space|3em>Store transition <math|<around*|(|s<rsub|t>,a<rsub|t>,r<rsub|t>,s<rsub|t+1>|)>>
    in <math|D>

    <space|3em>Sample random minibatch of transition
    <math|<around*|(|s<rsub|j>,a<rsub|j>,r<rsub|j>,s<rsub|j+1>|)>> from
    <math|D>

    <space|3em>Define <math|a<rsup|max><around*|(|s<rsub|j+1>;\<theta\>|)>=argmax<rsub|a<rprime|'>><around*|(|s<rsub|j+1>,a<rprime|'>;\<theta\>|)>>

    <space|3em>Set <math|y<rsub|j>=<choice|<tformat|<table|<row|<cell|r<rsub|j>>|<cell|<text|,
    if episode terminates at step >j+1>>|<row|<cell|r<rsub|j>+\<gamma\>
    max<rsub|a<rprime|'>> <wide|Q|^> <around*|(|s<rsub|j+1>,a<rsup|max><around*|(|s<rsub|j+1>;\<theta\>|)>;\<theta\><rsup|->|)>>|<cell|<text|,
    otherwise>>>>>>>

    <space|3em>Perform a gradient descent step on
    <math|<around*|(|y<rsub|j>-Q <around*|(|s<rsub|j>,a<rsub|j>;\<theta\>|)>|)><rsup|2>>
    w.r.t. the networ parameter <math|\<theta\>>

    <space|3em>Every <math|C> steps reset <math|<wide|Q|^>=Q>

    <space|1em><with|font-series|bold|End For>

    <with|font-series|bold|End For>
  </algorithm>

  <subsection|Dueling Deep Q-learning Network>

  We also employed another kind of improved DQN \V Dueling Deep Q-learning
  Network (Dueling DQN) to solving this problem. Dueling DQN is an extension
  of the original DQN algorithm. It introduces a modification to the DQN
  architecture, separating the estimation of the state-value and the
  advantage-value, which allows the agent to learn the value of each action
  independently from the state.\ 

  Instead of estimating the Q-value for each action directly, the Dueling DQN
  separates the estimation of the state-value <math|V<around*|(|s|)>> and the
  advantage-value <math|A<around*|(|s,a|)>>, where <math|V<around*|(|s|)>>
  represents the value of the state regardless of the action taken, and
  <math|A<around*|(|s,a|)>> represents the advantage of taking a certain
  action in a certain state. The Dueling DQN uses two separate streams in the
  neural network to estimate <math|V<around*|(|s|)>> and
  <math|A<around*|(|s,a|)>>, and combines them to obtain the final
  action-value function, <math|Q<around*|(|s,a|)>>, as the sum of
  <math|V<around*|(|s|)>> and <math|A<around*|(|s,a|)>> minus the mean of
  <math|A<around*|(|s,a|)>> across all actions, i.e.:

  <\equation*>
    Q<around*|(|s,a|)>=V<around*|(|s|)>+<around*|(|A<around*|(|s,a|)>-<frac|1|<around*|\||\<cal-A\>|\|>><big|sum><rsub|a<rprime|'>>A<around*|(|s,a<rprime|'>|)>|)>
  </equation*>

  where <math|<around*|\||\<cal-A\>|\|>> denoted the size of action space.
  The formal description is shown in Algorithm <reference|alg3>.

  <\algorithm>
    <label|alg3>Initialize replay memory <math|D> to capacity <math|N>

    Initialize action-value function <math|Q> with random weights
    <math|\<theta\>>

    Initialize target action-value function <math|<wide|Q|^>> with weights
    <math|\<theta\><rsup|->=\<theta\>>

    <with|font-series|bold|For> <math|episode=1,M> <with|font-series|bold|do>

    <space|1em>Initialize begining state <math|s<rsub|1>>

    <space|1em><with|font-series|bold|For> <math|t=1,T>
    <with|font-series|bold|do>

    <space|3em>With probability <math|\<epsilon\>> select a random action
    <math|a<rsub|t>>

    <space|3em>otherwise select <math|a<rsub|t>=argmax<rsub|a>
    Q<around*|(|\<phi\><around*|(|s<rsub|t>|)>,a;\<theta\>|)>>

    <space|3em>Execute action at in emulator and observe reward
    <math|r<rsub|t>> and next state <math|s<rsub|t+1>>

    <space|3em>Store transition <math|<around*|(|s<rsub|t>,a<rsub|t>,r<rsub|t>,s<rsub|t+1>|)>>
    in <math|D>

    <space|3em>Sample random minibatch of transition
    <math|<around*|(|s<rsub|j>,a<rsub|j>,r<rsub|j>,s<rsub|j+1>|)>> from
    <math|D>

    <space|3em>Define <math|a<rsup|max><around*|(|s<rsub|j+1>;\<theta\>|)>=argmax<rsub|a<rprime|'>><around*|(|s<rsub|j+1>,a<rprime|'>;\<theta\>|)>>

    <space|3em>Set <math|y<rsub|j>=<choice|<tformat|<table|<row|<cell|r<rsub|j>>|<cell|<text|,
    if episode terminates at step >j+1>>|<row|<cell|r<rsub|j>+\<gamma\>
    max<rsub|a<rprime|'>> <wide|Q|^> <around*|(|s<rsub|j+1>,a<rsup|max><around*|(|s<rsub|j+1>;\<theta\>|)>;\<theta\><rsup|->|)>>|<cell|<text|,
    otherwise>>>>>>>

    <space|3em>Perform a gradient descent step on
    <math|<around*|(|y<rsub|j>-Q <around*|(|s<rsub|j>,a<rsub|j>;\<theta\>|)>|)><rsup|2>>
    w.r.t. the networ parameter <math|\<theta\>>

    <space|3em>Every <math|C> steps reset <math|<wide|Q|^>=Q>

    <space|1em><with|font-series|bold|End For>

    <with|font-series|bold|End For>
  </algorithm>

  <section|Experiments>

  <subsection|Environment>

  We use the <with|font-family|tt|MountainCar-v0> environment provided by
  OpenAI gym<\footnote>
    <href|https://gymnasium.farama.org/environments/classic_control/mountain_car/>
  </footnote>. It consists of a car placed stochastically at the bottom of a
  sinusoidal valley, with the only possible actions being the accelerations
  that can be applied to the car in either direction.

  The observation space contains of two elements <math|x> and <math|v>,
  representing position of the car along the x-axis and velocity of the car,
  respectively. <math|x> is clipped to the range
  <math|<around*|[|-1.2,0.6|]>> and <math|v> is clipped to the range
  <math|<around*|[|-0.07,0.07|]>>.

  The action space contains three elements <math|<around*|{|0,1,2|}>>,
  representing accelerate to the left, don't accelerate and accelerate to the
  right, respectively.

  Given an action, the mountain car follows the following transition
  dynamics:

  <\eqnarray*>
    <tformat|<table|<row|<cell|>|<cell|v<rsub|t+1>=v<rsub|t>+<around*|(|a-1|)>\<cdot\>f-cos<around*|(|3\<cdot\>x<rsub|t>|)>\<cdot\>g>|<cell|>>|<row|<cell|>|<cell|x<rsub|t+1>=x<rsub|t>+v<rsub|t+1>>|<cell|>>>>
  </eqnarray*>

  where <math|a> denoted an action in the action space, <math|f> denoted the
  acceleration force and <math|g> denoted the gravity. By default,
  <math|f=0.001> and <math|g=0.0025>. The collisions at either end are
  inelastic with the velocity set to 0 upon collision with the wall.

  In the beginning, the position of the car is assigned a uniform random
  value in <math|<around*|[|-0.6,0.4|]>>. The starting velocity of the car is
  always assigned to 0. The goal is to reach the flag placed on top of the
  right hill as quickly as possible, as such the agent is penalised with a
  reward of -1 for each timestep. If the position of the car is greater than
  or equal to 0.5, then the episode end.

  <subsection|Training>

  \;

  <subsection|Results>

  In gym wiki<\footnote>
    <href|https://github.com/openai/gym/wiki/MountainCar-v0>
  </footnote>, MountainCar-v0 defines ``solving'' as getting average reward
  of -110.0 over 100 consecutive trials.

  <subsubsection|DQN Results>

  <small-figure|<image|file:///Users/yusen/Documents/Study/CS3316-\<#5F3A\>\<#5316\>\<#5B66\>\<#4E60\>/A/A4/images/dqn-reward.png|.4par|||>|1000
  episode><small-figure|<image|file:///Users/yusen/Documents/Study/CS3316-\<#5F3A\>\<#5316\>\<#5B66\>\<#4E60\>/A/A4/images/dqn-last-100-reward.png|.4par|||>|Last
  100 episodes>

  \;

  <subsubsection|Double DQN Results>

  <small-figure|<image|file:///Users/yusen/Documents/Study/CS3316-\<#5F3A\>\<#5316\>\<#5B66\>\<#4E60\>/A/A4/images/double-dqn-reward.png|.4par|||>|1000
  episode><small-figure|<image|file:///Users/yusen/Documents/Study/CS3316-\<#5F3A\>\<#5316\>\<#5B66\>\<#4E60\>/A/A4/images/double-dqn-last-100-reward.png|.4par|||>|Last
  100 episodes>

  <subsubsection|Dueling DQN Results>

  <small-figure|<image|file:///Users/yusen/Documents/Study/CS3316-\<#5F3A\>\<#5316\>\<#5B66\>\<#4E60\>/A/A4/images/dqn-reward.png|.4par|||>|1000
  episode><small-figure|<image|file:///Users/yusen/Documents/Study/CS3316-\<#5F3A\>\<#5316\>\<#5B66\>\<#4E60\>/A/A4/images/dqn-last-100-reward.png|.4par|||>|Last
  100 episodes>

  <section|Discussion and Conclusion>

  \;

  <appendix|Hyperparameters>

  \;
</body>

<\initial>
  <\collection>
    <associate|page-medium|paper>
    <associate|preamble|false>
  </collection>
</initial>

<\references>
  <\collection>
    <associate|alg1|<tuple|1|1>>
    <associate|alg2|<tuple|2|2>>
    <associate|alg3|<tuple|3|?>>
    <associate|auto-1|<tuple|1|1>>
    <associate|auto-10|<tuple|3.3.1|3>>
    <associate|auto-11|<tuple|1|3>>
    <associate|auto-12|<tuple|2|3>>
    <associate|auto-13|<tuple|3.3.2|3>>
    <associate|auto-14|<tuple|3|3>>
    <associate|auto-15|<tuple|4|3>>
    <associate|auto-16|<tuple|3.3.3|?>>
    <associate|auto-17|<tuple|5|?>>
    <associate|auto-18|<tuple|6|?>>
    <associate|auto-19|<tuple|4|?>>
    <associate|auto-2|<tuple|2|1>>
    <associate|auto-20|<tuple|A|?>>
    <associate|auto-3|<tuple|2.1|1>>
    <associate|auto-4|<tuple|2.2|2>>
    <associate|auto-5|<tuple|2.3|2>>
    <associate|auto-6|<tuple|3|2>>
    <associate|auto-7|<tuple|3.1|2>>
    <associate|auto-8|<tuple|3.2|2>>
    <associate|auto-9|<tuple|3.3|3>>
    <associate|footnote-1|<tuple|1|2>>
    <associate|footnote-2|<tuple|2|?>>
    <associate|footnr-1|<tuple|1|2>>
    <associate|footnr-2|<tuple|2|?>>
  </collection>
</references>

<\auxiliary>
  <\collection>
    <\associate|figure>
      <tuple|normal|<surround|<hidden-binding|<tuple>|1>||1000
      episode>|<pageref|auto-11>>

      <tuple|normal|<surround|<hidden-binding|<tuple>|2>||Last 100
      episodes>|<pageref|auto-12>>
    </associate>
    <\associate|toc>
      <vspace*|1fn><with|font-series|<quote|bold>|math-font-series|<quote|bold>|1<space|2spc>Introduction>
      <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-1><vspace|0.5fn>

      <vspace*|1fn><with|font-series|<quote|bold>|math-font-series|<quote|bold>|2<space|2spc>Methodology>
      <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-2><vspace|0.5fn>

      <with|par-left|<quote|1tab>|2.1<space|2spc>Deep Q-learning Network
      <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-3>>

      <with|par-left|<quote|1tab>|2.2<space|2spc>Dueling Deep Q-learning
      Network <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-4>>

      <vspace*|1fn><with|font-series|<quote|bold>|math-font-series|<quote|bold>|3<space|2spc>Experiments>
      <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-5><vspace|0.5fn>

      <with|par-left|<quote|1tab>|3.1<space|2spc>Environment
      <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-6>>

      <with|par-left|<quote|1tab>|3.2<space|2spc>Model Architecture
      <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-7>>

      <with|par-left|<quote|1tab>|3.3<space|2spc>Training
      <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-8>>

      <with|par-left|<quote|1tab>|3.4<space|2spc>Results
      <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-9>>

      <with|par-left|<quote|2tab>|3.4.1<space|2spc>DQN Results
      <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-10>>

      <with|par-left|<quote|2tab>|3.4.2<space|2spc>Dueling DQN Results
      <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-13>>

      <vspace*|1fn><with|font-series|<quote|bold>|math-font-series|<quote|bold>|4<space|2spc>Discussion
      and Conclusion> <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-14><vspace|0.5fn>

      <vspace*|1fn><with|font-series|<quote|bold>|math-font-series|<quote|bold>|Appendix
      A<space|2spc>Hyperparameters> <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-15><vspace|0.5fn>
    </associate>
  </collection>
</auxiliary>